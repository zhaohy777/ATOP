dataset:
  name: AES
  path: ../datasets/AES
  domains:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
  target_domain: 5
  adv: True
  adv_class: True
  kl_div: False

plm:
  model_name: bert
  model_path: ../bert
#  model_name: roberta
#  model_path: ../roberta-base
  optimize:
    freeze_para: True
    lr: 0.00003 #0.00001
    weight_decay: 0.01
    scheduler:
      type: 
      num_warmup_steps: 500

dataloader:
  max_seq_length: 512

train:
  batch_size: 4
  num_epochs: 30
  shuffle_data: True

test:
  batch_size: 32
  shuffle_data: False
dev:
  batch_size: 32
  shuffle_data: False

checkpoint:
  save_best: True


template: soft_manual_template
verbalizer: soft_verbalizer

soft_manual_template:
  choice: 3  # 无关提示长度10选3,长度5选4，长度1选5，长度15选6  # （10，10）选7
  file_path: ../scripts/TextClassification/mnli/soft_manual_template.txt
  optimize:
    lr: 0.003 #0.003
    weight_decay: 0.0
    scheduler:
      num_warmup_steps: 0

manual_verbalizer:
  choice: 0
  file_path: ../scripts/TextClassification/mnli/multiwords_verbalizer.jsonl

soft_verbalizer:
  choice: 1
#  file_path: ../scripts/TextClassification/mnli/multiwords_verbalizer.jsonl


environment:
  num_gpus: 4
  cuda_visible_devices:
    - 0
    - 1
    - 2
    - 3
  local_rank: 0

learning_setting: full
logging:
  path: ../yaml

#few_shot:
#  parent_config: learning_setting
#  few_shot_sampling: sampling_from_train

#sampling_from_train:
#  parent_config: few_shot_sampling
#  num_examples_per_label: 10
#  also_sample_dev: True
#  num_examples_per_label_dev: 10
#  seed: 123

task: classification
classification:
  parent_config: task
  metric:  # the first one will be the main  to determine checkpoint.
    - accuracy  # whether the higher metric value is better.
  loss_function: cross_entropy ## the loss function for classification
